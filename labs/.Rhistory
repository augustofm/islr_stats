#Gini index
p <- seq(0,1,0.01)
#Gini index
pm1 <- seq(0,1,0.01)
pm2 <- 1 - pm1
p.dt <- data.table(pm1 = pm1,
pm2 = pm2)
## Question 03 ----
require(data.table)
#Gini index
pm1 <- seq(0,1,0.01)
pm2 <- 1 - pm1
p.dt <- data.table(pm1 = pm1,
pm2 = pm2)
p.dt[, G := sum(pm1*(1-pm2))]
p.dt[, G := pm1*(1-pm2)+pm2*(1-pm1)]
#Entropy
p.dt[, D := -(pm1*log(pm1)+pm2*log(pm2))]
#Classification error
p.dt[, E := 1 - max(pm1, pm2)]
plot(p.dt$E)
p.dt
#Classification error
p.dt[, E := 1 - max(pm1, pm2), by = pm1]
p.dt[, E]
require(ggplot2)
p.dt.melt <- melt(p.dt, id = c("pm1", "pm2"))
p.dt.melt
ggplot(p.dt.melt)+geoom_line(aes(x=pm1, y=value, color=variable))+
theme_classic()
ggplot(p.dt.melt)+
geom_line(aes(x=pm1, y=value, color=variable))+
theme_classic()
ggplot(p.dt.melt)+
geom_line(aes(x=pm1, y=value, color=variable))+
theme_classic()+
labs(x="Probability of class 1",
y="Measurement")
## Question 05 ----
p <- c(0.1, 0.15, 0.2, 0.2, 0.55,
0.6, 0.6, 0.65, 0.7, 0.75)
# Approach 1:
#Not RED
length(p[p<0.5])
#RED
length(p[p>=0.5])
# So, the class would be Red
# Approach 2:
mean(p)
### Applied ----
library(ISLR)
library(tree)
library(trees)
install.packages("tree")
# install.packages("tree")
library(tree)
dt <- data.table(Carseats)
dt[, High := ifelse(Sales<=8, "No","Yes")]
tree.carseats <- tree(High ~. -Sales, data = dt)
dt
summary(tree.carseats)
dt <- data.table(Carseats)
dt[, High := ifelse(Sales>8, "Yes","No")]
tree.carseats <- tree(High ~ .-Sales, data = dt)
summary(tree.carseats)
dt <- data.table(Carseats)
dt[, High := ifelse(Sales>8, "Yes","No")]
tree.carseats <- tree(High ~ .-Sales, data = dt)
summary(tree.carseats)
tree.carseats
dt <- na.omit(dt)
dt[, High := ifelse(Sales>8, "Yes","No")]
dt <- na.omit(dt)
tree.carseats <- tree(High ~ .-Sales, data = dt)
tree.carseats <- tree(High ~ .-Sales, dt)
attach(dt)
tree.carseats <- tree(High ~ .-Sales, dt)
str(dt)
dt[, High := as.factor(ifelse(Sales>8, "Yes","No"))]
tree.carseats <- tree(High ~ .-Sales, dt)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
train <- sample(1:nrow(dt))
train <- sample(1:nrow(dt), 200)
dt.test <- dt[-train]
dt.test <- dt[-train,]
High.test <- dt.test$High
tree.carseats <- tree(High ~. - Sales, data = dt, subset = train)
tree.pred <- predict(tree.carseats, dt.test, type = "class")
table(tree.pred, High.test)
set.seed(2)
train <- sample(1:nrow(dt), 200)
dt.test <- dt[-train,]
High.test <- dt.test$High
tree.carseats <- tree(High ~. - Sales, data = dt, subset = train)
tree.pred <- predict(tree.carseats, dt.test, type = "class")
table(tree.pred, High.test)
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
?cv.carseats
?cv.tree
cv.tree(tree.carseats, FUN = prune.misclass)
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
tree.carseats <- tree(High ~. - Sales, data = Carseats, subset = train)
tree.carseats <- tree(High ~. - Sales, data = dt, subset = train)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, dt.test, type = "class")
table(tree.pred, High.test)
## 8.3.2 Fitting Regression Trees
library(MASS)
set.seed(1)
dt <- data.table(Boston)
train <- sample(1:nrow(dt), 0.5*nrow(dt))
tree.boston <- tree(medv ~., data = dt, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
cv.boston <- cv.tree(tree.boston)
# If pruned
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat <- predict(tree.boston, newdata = dt[-train,])
boston.test <- dt[-train, medv]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
## 8.3.3 Bagging and Random Forests
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ .,
dt,
subset = train)
bag.boston <- randomForest(medv ~ .,
dt,
subset = train,
mtry = 13,
importance = TRUE)
bag.boston
yhat.bag <- predict(bag.boston, dt[-train,])
plot(yhat.bag, boston.test)
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag - boston.test)-2)
mean((yhat.bag - boston.test)^2)
bag.boston <- randomForest(medv ~ .,
dt,
subset = train,
mtry = 13,
importance = TRUE,
ntree = 25)
yhat.bag <- predict(bag.boston, dt[-train,])
mean((yhat.bag - boston.test)^2)
set.seed(1)
rf.boston <- randomForest(medv ~ .,
dt,
subset = train,
mtry = 6,
importance = T)
yhat.rf <- predict(bag.boston, dt[-train,])
mean((yhat.rf - boston.test)^2)
yhat.rf <- predict(rf.boston, dt[-train,])
mean((yhat.rf - boston.test)^2)
importance(rf.boston)
# rf performed better then bagging
varImpPlot(rf.boston)
library(gbm)
install.packages("gbm")
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = dt[train,],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4)
summary(boost.boston)
par(mfrow = c(1,2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
yhat.boost <- predict(boost.boston,
newdata = dt[-train,],
n.trees = 5000)
mean((yhat.boost-boston.test)^2)
boost.boston <- gbm(medv ~ ., data = dt[train,],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.2,
verbose = F)
yhat.boost <- predict(boost.boston,
newdata = dt[-train,],
n.trees = 5000)
mean((yhat.boost-boston.test)^2)
